{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sem_10_summarization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws1FXPusI0Bh"
      },
      "source": [
        "# Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTgTMQb8M0aN"
      },
      "source": [
        "## Deep Reinforced Model for Abstractive Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVan4a0tNCJY"
      },
      "source": [
        "[A Deep Reinforced Model for Abstractive Summarization (Romain Paulus, Caiming Xiong, Richard Socher, 2017)](https://arxiv.org/abs/1705.04304) - модель суммаризации на основе encoder-decoder с использованием reinforcement learning для обучения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUtEp0ZlS6xK"
      },
      "source": [
        "### Источники\n",
        "1. Блог с описанием статьи: https://blog.einstein.ai/your-tldr-by-an-ai-a-deep-reinforced-model-for-abstractive-summarization/\n",
        "2. Имплементация: https://github.com/rohithreddy024/Text-Summarizer-Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--9IWPY_gK-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12a84cab-16f7-47fe-ca32-1d2045ca41b5"
      },
      "source": [
        "!git clone https://github.com/rohithreddy024/Text-Summarizer-Pytorch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Text-Summarizer-Pytorch'...\n",
            "remote: Enumerating objects: 98, done.\u001b[K\n",
            "remote: Total 98 (delta 0), reused 0 (delta 0), pack-reused 98\u001b[K\n",
            "Unpacking objects: 100% (98/98), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDDCVRinJUgF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "24f7827d-e463-499a-b38a-cad77a5fe17c"
      },
      "source": [
        "!pip2 install tensorflow"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/73/205b5e7f8fe086ffe4165d984acb2c49fa3086f330f03099378753982d2e/tensorflow-2.1.0-cp27-cp27mu-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 34kB/s \n",
            "\u001b[?25hRequirement already satisfied: gast==0.2.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.16.4)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: scipy==1.2.2; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.2.2)\n",
            "Requirement already satisfied: wheel; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.11.2)\n",
            "Collecting keras-preprocessing>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: backports.weakref>=1.0rc1; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.0.post1)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 45.4MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/56/4bcec5a8d9503a87e58e814c4e32ac2b32c37c685672c30bc8c54c6e478a/Keras_Applications-1.0.8.tar.gz (289kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 46.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: functools32>=3.2.3; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (3.2.3.post2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.7.1)\n",
            "Collecting opt-einsum>=2.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/d6/44792ec668bcda7d91913c75237314e688f70415ab2acd7172c845f0b24f/opt_einsum-2.3.2.tar.gz (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.3MB/s \n",
            "\u001b[?25hCollecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/c5/94a66686b86adfb3ef6f34837d0a7cc2efdc995bc39cad64a9b3e103f0d5/tensorboard-2.1.0-py2-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 33.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.1.7)\n",
            "Requirement already satisfied: enum34>=1.1.6; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.1.10)\n",
            "Requirement already satisfied: mock>=2.0.0; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.8.0->tensorflow) (44.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.15.5)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/7a/1b3eb54caee1b8c73c2c3645f78a382eca4805a301a30c64a078e736e446/google_auth-1.35.0-py2.py3-none-any.whl (152kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 38.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0; python_version < \"3\"->tensorflow) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0; python_version < \"3\"->tensorflow) (5.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.5)\n",
            "Requirement already satisfied: rsa<4.6; python_version < \"3.6\" in /usr/local/lib/python2.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.6.16)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.1 in /usr/local/lib/python2.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.5)\n",
            "Building wheels for collected packages: keras-applications, opt-einsum\n",
            "  Building wheel for keras-applications (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-applications: filename=Keras_Applications-1.0.8-cp27-none-any.whl size=50704 sha256=5f5948f0f5be02572447d2d6885972d401992d0d9e9d02dce94a7db34156b44e\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/f2/5d/2689b5547f32c4e258c3b7ccbe7f1d0f2afbb84fb01e830792\n",
            "  Building wheel for opt-einsum (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for opt-einsum: filename=opt_einsum-2.3.2-cp27-none-any.whl size=49884 sha256=dc11b71ee437ba06b730e293103e8b33acff40856e57bccbb1bb727fbc3654d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/3e/a3/b351fae0cbf15373c2136a54a70f43fea5fe91d8168a5faaa4\n",
            "Successfully built keras-applications opt-einsum\n",
            "\u001b[31mERROR: tensorboard 2.1.0 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-preprocessing, tensorflow-estimator, keras-applications, opt-einsum, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Found existing installation: google-auth 2.3.3\n",
            "    Uninstalling google-auth-2.3.3:\n",
            "      Successfully uninstalled google-auth-2.3.3\n",
            "  Found existing installation: google-auth-oauthlib 0.4.0\n",
            "    Uninstalling google-auth-oauthlib-0.4.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.0\n",
            "Successfully installed google-auth-1.35.0 google-auth-oauthlib-0.4.1 keras-applications-1.0.8 keras-preprocessing-1.1.2 opt-einsum-2.3.2 tensorboard-2.1.0 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE8B7lXJ_TGl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fbe033ba-fe5e-431b-c89a-1ece21673157"
      },
      "source": [
        "!pip install tensorflow==2.3.0\n",
        "from tensorflow import core"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.3.0\n",
            "  Downloading tensorflow-2.3.0-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4 MB 46 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.13.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (3.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.37.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (2.7.0)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 37.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.12.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.1.2)\n",
            "Collecting numpy<1.19.0,>=1.16.0\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 24.9 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.4.0,>=2.3.0\n",
            "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
            "\u001b[K     |████████████████████████████████| 459 kB 42.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.4.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (3.17.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.42.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (57.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.1.1)\n",
            "Installing collected packages: numpy, tensorflow-estimator, h5py, gast, tensorflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.3.3 h5py-2.10.0 numpy-1.18.5 tensorflow-2.3.0 tensorflow-estimator-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSI7sgTDiks1"
      },
      "source": [
        "### Данные \n",
        "Модель обучается на данных Gigaword dataset: https://data.deepai.org/gigaword.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUWvD95NVaTZ"
      },
      "source": [
        "# find the share link of the file/folder on Google Drive\n",
        "#file_share_link = \"https://drive.google.com/open?id=0B6N7tANPyVeBNmlSX19Ld2xDU1E\"\n",
        "\n",
        "# extract the ID of the file\n",
        "#file_id = file_share_link[file_share_link.find(\"=\") + 1:]\n",
        "\n",
        "# append the id to this REST command\n",
        "#file_download_link = \"https://docs.google.com/uc?export=download&id=\" + file_id "
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe4k_BLYYJot"
      },
      "source": [
        "#file_id"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYOUzkNGXl4t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8864a577-bde1-476b-f9ad-ad9c516176fb"
      },
      "source": [
        "!wget https://data.deepai.org/gigaword.zip "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-22 07:58:29--  https://data.deepai.org/gigaword.zip\n",
            "Resolving data.deepai.org (data.deepai.org)... 138.201.36.183\n",
            "Connecting to data.deepai.org (data.deepai.org)|138.201.36.183|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 297054860 (283M) [application/x-zip-compressed]\n",
            "Saving to: ‘gigaword.zip’\n",
            "\n",
            "gigaword.zip        100%[===================>] 283.29M  96.1MB/s    in 2.9s    \n",
            "\n",
            "2021-11-22 07:58:32 (96.1 MB/s) - ‘gigaword.zip’ saved [297054860/297054860]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdA8FJ-YT3Jl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9c5a3e6-7449-4838-bb28-58bdec5a3194"
      },
      "source": [
        "#!tar -tvf summary.tar.gz\n",
        "!unzip gigaword.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  gigaword.zip\n",
            "  inflating: sumdata/DUC2003/input.txt  \n",
            "  inflating: sumdata/DUC2003/task1_ref0.txt  \n",
            "  inflating: sumdata/DUC2003/task1_ref1.txt  \n",
            "  inflating: sumdata/DUC2003/task1_ref2.txt  \n",
            "  inflating: sumdata/DUC2003/task1_ref3.txt  \n",
            "  inflating: sumdata/DUC2004/input.txt  \n",
            "  inflating: sumdata/DUC2004/task1_ref0.txt  \n",
            "  inflating: sumdata/DUC2004/task1_ref1.txt  \n",
            "  inflating: sumdata/DUC2004/task1_ref2.txt  \n",
            "  inflating: sumdata/DUC2004/task1_ref3.txt  \n",
            "  inflating: sumdata/Giga/input.txt  \n",
            "  inflating: sumdata/Giga/task1_ref0.txt  \n",
            "  inflating: sumdata/train/train.article.txt  \n",
            "  inflating: sumdata/train/train.title.txt  \n",
            "  inflating: sumdata/train/valid.article.filter.txt  \n",
            "  inflating: sumdata/train/valid.title.filter.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atSkLKu7YUbo"
      },
      "source": [
        "!mv sumdata/train/* Text-Summarizer-Pytorch/data/unfinished\n",
        "!rm -rf sumdata/"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPtC-lUgg60A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b5856df-98e1-4b7c-a5aa-dff45d39eb9e"
      },
      "source": [
        "%cd Text-Summarizer-Pytorch"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Text-Summarizer-Pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULtgqMeehNiv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c20a4d6-07f9-46da-accd-75f4af6a3769"
      },
      "source": [
        "# Создание .bin файлов с данными для обучения модели\n",
        "!python2 make_data_files.py"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-11-22 07:59:25.726372: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2021-11-22 07:59:25.727230: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2021-11-22 07:59:25.727258: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Completed shuffling train & valid text files\n",
            "3803957it [03:29, 18175.27it/s]\n",
            "189651it [00:03, 49760.08it/s]\n",
            "Completed creating bin file for train & valid\n",
            "Completed chunking main bin files into smaller ones\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK3MRVgZ2O5H"
      },
      "source": [
        "Примеры обучающих данных (заголовки и абстракты статей):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VmxqtFmzKIO"
      },
      "source": [
        "N = 20\n",
        "with open(\"data/unfinished/train.article.txt\") as f:\n",
        "    head = [next(f) for x in range(N)]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0El6aWazoUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f9be21b-0d6c-41fe-b57f-84650df2f93f"
      },
      "source": [
        "print(head[1])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "at least two people were killed in a suspected bomb attack on a passenger bus in the strife-torn southern philippines on monday , the military said .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH46yo-8z1G4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5034da81-a540-479c-d540-672ec431c4fc"
      },
      "source": [
        "N = 20\n",
        "with open(\"data/unfinished/train.title.txt\") as f:\n",
        "    head = [next(f) for x in range(N)]\n",
        "print(head[1])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "at least two dead in southern philippines blast\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woOtaany2yVS"
      },
      "source": [
        "### Модель\n",
        "Идея состоит в обучении encoder-decoder архитектуры для генерации summary входного текста. \n",
        "\n",
        "В декодере дважды используется механизм attention:\n",
        "1. attention на состояния энкодера (intra-temporal attention) определяет вес слов входной последовательности для текущей позиции в выходной последовательности summary\n",
        "2. attention на предыдущие состояния декодера (intra-decoder attention) для того, чтобы не допускать повторения слов в выходе декодера.\n",
        "\n",
        "В процессе обучения модели используется teacher-forcing, чтобы учитывать ошибку на уровне каждого генерируемого слова (Negative Log Likelihood Loss), и reinforcement learning для оценки качества сгенерированного текста целиком в сравнении с target summary. \n",
        "\n",
        "Для reinforcement learning в качестве метрики используется ROUGE score. ROUGE считает совпадение н-грамм слов в таргете и сгенерированной последовательности (ROUGE-1 для униграмм, ROUGE-2 для биграмм слов, ...). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD-rqVkqfIxF"
      },
      "source": [
        "![summ_attentions](summ-attentions.svg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKDtgnWLmxlg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a56860a-3d8a-4ba6-c657-b3091d283d31"
      },
      "source": [
        "# rouge для подчета метрики Rouge\n",
        "!pip install rouge"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c7KO5v3C730"
      },
      "source": [
        "Обучение модели (train.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCMPOhhRCZ7Z"
      },
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"    #Set cuda device\n",
        "\n",
        "import time\n",
        "\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from model import Model\n",
        "\n",
        "from data_util import config, data\n",
        "from data_util.batcher import Batcher\n",
        "from data_util.data import Vocab\n",
        "from train_util import *\n",
        "from torch.distributions import Categorical\n",
        "from rouge import Rouge\n",
        "from numpy import random\n",
        "import argparse\n",
        "\n",
        "random.seed(123)\n",
        "T.manual_seed(123)\n",
        "if T.cuda.is_available():\n",
        "    T.cuda.manual_seed_all(123)\n",
        "\n",
        "class Train(object):\n",
        "    def __init__(self, opt):\n",
        "        self.vocab = Vocab(config.vocab_path, config.vocab_size)\n",
        "        self.batcher = Batcher(config.train_data_path, self.vocab, mode='train',\n",
        "                               batch_size=config.batch_size, single_pass=False)\n",
        "        self.opt = opt\n",
        "        self.start_id = self.vocab.word2id(data.START_DECODING)\n",
        "        self.end_id = self.vocab.word2id(data.STOP_DECODING)\n",
        "        self.pad_id = self.vocab.word2id(data.PAD_TOKEN)\n",
        "        self.unk_id = self.vocab.word2id(data.UNKNOWN_TOKEN)\n",
        "        time.sleep(5)\n",
        "\n",
        "    def save_model(self, iter):\n",
        "        save_path = config.save_model_path + \"/%07d.tar\" % iter\n",
        "        T.save({\n",
        "            \"iter\": iter + 1,\n",
        "            \"model_dict\": self.model.state_dict(),\n",
        "            \"trainer_dict\": self.trainer.state_dict()\n",
        "        }, save_path)\n",
        "\n",
        "    def setup_train(self):\n",
        "        self.model = Model()\n",
        "        self.model = get_cuda(self.model)\n",
        "        self.trainer = T.optim.Adam(self.model.parameters(), lr=config.lr)\n",
        "        start_iter = 0\n",
        "        if self.opt.load_model is not None:\n",
        "            load_model_path = os.path.join(config.save_model_path, self.opt.load_model)\n",
        "            checkpoint = T.load(load_model_path)\n",
        "            start_iter = checkpoint[\"iter\"]\n",
        "            self.model.load_state_dict(checkpoint[\"model_dict\"])\n",
        "            self.trainer.load_state_dict(checkpoint[\"trainer_dict\"])\n",
        "            print(\"Loaded model at \" + load_model_path)\n",
        "        if self.opt.new_lr is not None:\n",
        "            self.trainer = T.optim.Adam(self.model.parameters(), lr=self.opt.new_lr)\n",
        "        return start_iter\n",
        "\n",
        "    def train_batch_MLE(self, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, batch):\n",
        "        ''' Calculate Negative Log Likelihood Loss for the given batch. In order to reduce exposure bias,\n",
        "                pass the previous generated token as input with a probability of 0.25 instead of ground truth label\n",
        "        Args:\n",
        "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
        "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
        "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
        "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
        "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
        "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
        "        :param batch: batch object\n",
        "        '''\n",
        "        dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(batch)                        #Get input and target batchs for training decoder\n",
        "        step_losses = []\n",
        "        s_t = (enc_hidden[0], enc_hidden[1])                                                        #Decoder hidden states\n",
        "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(self.start_id))                             #Input to the decoder\n",
        "        prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
        "        sum_temporal_srcs = None                                                                    #Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
        "        for t in range(min(max_dec_len, config.max_dec_steps)):\n",
        "            use_gound_truth = get_cuda((T.rand(len(enc_out)) > 0.25)).long()                        #Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
        "            x_t = use_gound_truth * dec_batch[:, t] + (1 - use_gound_truth) * x_t                   #Select decoder input based on use_ground_truth probabilities\n",
        "            x_t = self.model.embeds(x_t)\n",
        "            final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = self.model.decoder(x_t, s_t, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n",
        "            target = target_batch[:, t]\n",
        "            log_probs = T.log(final_dist + config.eps)\n",
        "            step_loss = F.nll_loss(log_probs, target, reduction=\"none\", ignore_index=self.pad_id)\n",
        "            step_losses.append(step_loss)\n",
        "            x_t = T.multinomial(final_dist, 1).squeeze()                                            #Sample words from final distribution which can be used as input in next time step\n",
        "            is_oov = (x_t >= config.vocab_size).long()                                              #Mask indicating whether sampled word is OOV\n",
        "            x_t = (1 - is_oov) * x_t.detach() + (is_oov) * self.unk_id                              #Replace OOVs with [UNK] token\n",
        "\n",
        "        losses = T.sum(T.stack(step_losses, 1), 1)                                                  #unnormalized losses for each example in the batch; (batch_size)\n",
        "        batch_avg_loss = losses / dec_lens                                                          #Normalized losses; (batch_size)\n",
        "        mle_loss = T.mean(batch_avg_loss)                                                           #Average batch loss\n",
        "        return mle_loss\n",
        "\n",
        "    def train_batch_RL(self, enc_out, enc_hidden, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, article_oovs, greedy):\n",
        "        '''Generate sentences from decoder entirely using sampled tokens as input. These sentences are used for ROUGE evaluation\n",
        "        Args\n",
        "        :param enc_out: Outputs of the encoder for all time steps (batch_size, length_input_sequence, 2*hidden_size)\n",
        "        :param enc_hidden: Tuple containing final hidden state & cell state of encoder. Shape of h & c: (batch_size, hidden_size)\n",
        "        :param enc_padding_mask: Mask for encoder input; Tensor of size (batch_size, length_input_sequence) with values of 0 for pad tokens & 1 for others\n",
        "        :param ct_e: encoder context vector for time_step=0 (eq 5 in https://arxiv.org/pdf/1705.04304.pdf)\n",
        "        :param extra_zeros: Tensor used to extend vocab distribution for pointer mechanism\n",
        "        :param enc_batch_extend_vocab: Input batch that stores OOV ids\n",
        "        :param article_oovs: Batch containing list of OOVs in each example\n",
        "        :param greedy: If true, performs greedy based sampling, else performs multinomial sampling\n",
        "        Returns:\n",
        "        :decoded_strs: List of decoded sentences\n",
        "        :log_probs: Log probabilities of sampled words\n",
        "        '''\n",
        "        s_t = enc_hidden                                                                            #Decoder hidden states\n",
        "        x_t = get_cuda(T.LongTensor(len(enc_out)).fill_(self.start_id))                             #Input to the decoder\n",
        "        prev_s = None                                                                               #Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
        "        sum_temporal_srcs = None                                                                    #Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
        "        inds = []                                                                                   #Stores sampled indices for each time step\n",
        "        decoder_padding_mask = []                                                                   #Stores padding masks of generated samples\n",
        "        log_probs = []                                                                              #Stores log probabilites of generated samples\n",
        "        mask = get_cuda(T.LongTensor(len(enc_out)).fill_(1))                                        #Values that indicate whether [STOP] token has already been encountered; 1 => Not encountered, 0 otherwise\n",
        "\n",
        "        for t in range(config.max_dec_steps):\n",
        "            x_t = self.model.embeds(x_t)\n",
        "            probs, s_t, ct_e, sum_temporal_srcs, prev_s = self.model.decoder(x_t, s_t, enc_out, enc_padding_mask, ct_e, extra_zeros, enc_batch_extend_vocab, sum_temporal_srcs, prev_s)\n",
        "            if greedy is False:\n",
        "                multi_dist = Categorical(probs)\n",
        "                x_t = multi_dist.sample()                                                           #perform multinomial sampling\n",
        "                log_prob = multi_dist.log_prob(x_t)\n",
        "                log_probs.append(log_prob)\n",
        "            else:\n",
        "                _, x_t = T.max(probs, dim=1)                                                        #perform greedy sampling\n",
        "            x_t = x_t.detach()\n",
        "            inds.append(x_t)\n",
        "            mask_t = get_cuda(T.zeros(len(enc_out)))                                                #Padding mask of batch for current time step\n",
        "            mask_t[mask == 1] = 1                                                                   #If [STOP] is not encountered till previous time step, mask_t = 1 else mask_t = 0\n",
        "            mask[(mask == 1) + (x_t == self.end_id) == 2] = 0                                       #If [STOP] is not encountered till previous time step and current word is [STOP], make mask = 0\n",
        "            decoder_padding_mask.append(mask_t)\n",
        "            is_oov = (x_t>=config.vocab_size).long()                                                #Mask indicating whether sampled word is OOV\n",
        "            x_t = (1-is_oov)*x_t + (is_oov)*self.unk_id                                             #Replace OOVs with [UNK] token\n",
        "\n",
        "        inds = T.stack(inds, dim=1)\n",
        "        decoder_padding_mask = T.stack(decoder_padding_mask, dim=1)\n",
        "        if greedy is False:                                                                         #If multinomial based sampling, compute log probabilites of sampled words\n",
        "            log_probs = T.stack(log_probs, dim=1)\n",
        "            log_probs = log_probs * decoder_padding_mask                                            #Not considering sampled words with padding mask = 0\n",
        "            lens = T.sum(decoder_padding_mask, dim=1)                                               #Length of sampled sentence\n",
        "            log_probs = T.sum(log_probs, dim=1) / lens  # (bs,)                                     #compute normalizied log probability of a sentence\n",
        "        decoded_strs = []\n",
        "        for i in range(len(enc_out)):\n",
        "            id_list = inds[i].cpu().numpy()\n",
        "            oovs = article_oovs[i]\n",
        "            S = data.outputids2words(id_list, self.vocab, oovs)                                     #Generate sentence corresponding to sampled words\n",
        "            try:\n",
        "                end_idx = S.index(data.STOP_DECODING)\n",
        "                S = S[:end_idx]\n",
        "            except ValueError:\n",
        "                S = S\n",
        "            if len(S) < 2:                                                                           #If length of sentence is less than 2 words, replace it with \"xxx\"; Avoids setences like \".\" which throws error while calculating ROUGE\n",
        "                S = [\"xxx\"]\n",
        "            S = \" \".join(S)\n",
        "            decoded_strs.append(S)\n",
        "\n",
        "        return decoded_strs, log_probs\n",
        "\n",
        "    def reward_function(self, decoded_sents, original_sents):\n",
        "        rouge = Rouge()\n",
        "        try:\n",
        "            scores = rouge.get_scores(decoded_sents, original_sents)\n",
        "        except Exception:\n",
        "            print(\"Rouge failed for multi sentence evaluation.. Finding exact pair\")\n",
        "            scores = []\n",
        "            for i in range(len(decoded_sents)):\n",
        "                try:\n",
        "                    score = rouge.get_scores(decoded_sents[i], original_sents[i])\n",
        "                except Exception:\n",
        "                    print(\"Error occured at:\")\n",
        "                    print(\"decoded_sents:\", decoded_sents[i])\n",
        "                    print(\"original_sents:\", original_sents[i])\n",
        "                    score = [{\"rouge-l\":{\"f\":0.0}}]\n",
        "                scores.append(score[0])\n",
        "        rouge_l_f1 = [score[\"rouge-l\"][\"f\"] for score in scores]\n",
        "        rouge_l_f1 = get_cuda(T.FloatTensor(rouge_l_f1))\n",
        "        return rouge_l_f1\n",
        "\n",
        "    # def write_to_file(self, decoded, max, original, sample_r, baseline_r, iter):\n",
        "    #     with open(\"temp.txt\", \"w\") as f:\n",
        "    #         f.write(\"iter:\"+str(iter)+\"\\n\")\n",
        "    #         for i in range(len(original)):\n",
        "    #             f.write(\"dec: \"+decoded[i]+\"\\n\")\n",
        "    #             f.write(\"max: \"+max[i]+\"\\n\")\n",
        "    #             f.write(\"org: \"+original[i]+\"\\n\")\n",
        "    #             f.write(\"Sample_R: %.4f, Baseline_R: %.4f\\n\\n\"%(sample_r[i].item(), baseline_r[i].item()))\n",
        "\n",
        "\n",
        "    def train_one_batch(self, batch, iter):\n",
        "        enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, context = get_enc_data(batch)\n",
        "\n",
        "        enc_batch = self.model.embeds(enc_batch)                                                    #Get embeddings for encoder input\n",
        "        enc_out, enc_hidden = self.model.encoder(enc_batch, enc_lens)\n",
        "\n",
        "        # -------------------------------Summarization-----------------------\n",
        "        if self.opt.train_mle == \"yes\":                                                             #perform MLE training\n",
        "            mle_loss = self.train_batch_MLE(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, batch)\n",
        "        else:\n",
        "            mle_loss = get_cuda(T.FloatTensor([0]))\n",
        "        # --------------RL training-----------------------------------------------------\n",
        "        if self.opt.train_rl == \"yes\":                                                              #perform reinforcement learning training\n",
        "            # multinomial sampling\n",
        "            sample_sents, RL_log_probs = self.train_batch_RL(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, batch.art_oovs, greedy=False)\n",
        "            with T.autograd.no_grad():\n",
        "                # greedy sampling\n",
        "                greedy_sents, _ = self.train_batch_RL(enc_out, enc_hidden, enc_padding_mask, context, extra_zeros, enc_batch_extend_vocab, batch.art_oovs, greedy=True)\n",
        "\n",
        "            sample_reward = self.reward_function(sample_sents, batch.original_abstracts)\n",
        "            baseline_reward = self.reward_function(greedy_sents, batch.original_abstracts)\n",
        "            # if iter%200 == 0:\n",
        "            #     self.write_to_file(sample_sents, greedy_sents, batch.original_abstracts, sample_reward, baseline_reward, iter)\n",
        "            rl_loss = -(sample_reward - baseline_reward) * RL_log_probs                             #Self-critic policy gradient training (eq 15 in https://arxiv.org/pdf/1705.04304.pdf)\n",
        "            rl_loss = T.mean(rl_loss)\n",
        "\n",
        "            batch_reward = T.mean(sample_reward).item()\n",
        "        else:\n",
        "            rl_loss = get_cuda(T.FloatTensor([0]))\n",
        "            batch_reward = 0\n",
        "\n",
        "    # ------------------------------------------------------------------------------------\n",
        "        self.trainer.zero_grad()\n",
        "        (self.opt.mle_weight * mle_loss + self.opt.rl_weight * rl_loss).backward()\n",
        "        self.trainer.step()\n",
        "\n",
        "        return mle_loss.item(), batch_reward\n",
        "\n",
        "    def trainIters(self):\n",
        "        iter = self.setup_train()\n",
        "        count = mle_total = r_total = 0\n",
        "        while iter <= config.max_iterations:\n",
        "            batch = self.batcher.next_batch()\n",
        "            try:\n",
        "                mle_loss, r = self.train_one_batch(batch, iter)\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"-------------------Keyboard Interrupt------------------\")\n",
        "                exit(0)\n",
        "\n",
        "            mle_total += mle_loss\n",
        "            r_total += r\n",
        "            count += 1\n",
        "            iter += 1\n",
        "\n",
        "            if iter % 1000 == 0:\n",
        "                mle_avg = mle_total / count\n",
        "                r_avg = r_total / count\n",
        "                print(\"iter:\", iter, \"mle_loss:\", \"%.3f\" % mle_avg, \"reward:\", \"%.4f\" % r_avg)\n",
        "                count = mle_total = r_total = 0\n",
        "\n",
        "            if iter % 5000 == 0:\n",
        "                self.save_model(iter)\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument('--train_mle', type=str, default=\"yes\")\n",
        "# parser.add_argument('--train_rl', type=str, default=\"no\")\n",
        "# parser.add_argument('--mle_weight', type=float, default=1.0)\n",
        "# parser.add_argument('--load_model', type=str, default=None)\n",
        "# parser.add_argument('--new_lr', type=float, default=None)\n",
        "# opt = parser.parse_args()\n",
        "# opt.rl_weight = 1 - opt.mle_weight\n",
        "# print(\"Training mle: %s, Training rl: %s, mle weight: %.2f, rl weight: %.2f\"%(opt.train_mle, opt.train_rl, opt.mle_weight, opt.rl_weight))\n",
        "# print(\"intra_encoder:\", config.intra_encoder, \"intra_decoder:\", config.intra_decoder)\n",
        "\n",
        "# train_processor = Train(opt)\n",
        "# train_processor.trainIters()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4SsF_7BDQwF"
      },
      "source": [
        "Сначала encoder-decoder модель обучается без reinforcement learning. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kUSZGFojE9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b38132b-2ef7-4aec-ed65-cd12fecb5f53"
      },
      "source": [
        "!python train.py --train_mle=yes --train_rl=no --mle_weight=1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-11-19 11:53:58.057665: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Training mle: yes, Training rl: no, mle weight: 1.00, rl weight: 0.00\n",
            "intra_encoder: True intra_decoder: True\n",
            "Exception in thread Thread-3:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/content/Text-Summarizer-Pytorch/data_util/batcher.py\", line 247, in watch_threads\n",
            "    tf.logging.info(\n",
            "AttributeError: module 'tensorflow' has no attribute 'logging'\n",
            "\n",
            "iter: 1000 mle_loss: 8.687 reward: 0.0000\n",
            "iter: 2000 mle_loss: 8.463 reward: 0.0000\n",
            "iter: 3000 mle_loss: 8.542 reward: 0.0000\n",
            "iter: 4000 mle_loss: 8.457 reward: 0.0000\n",
            "iter: 5000 mle_loss: 8.399 reward: 0.0000\n",
            "-------------------Keyboard Interrupt------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r049heCmGAIT"
      },
      "source": [
        "Выбирается лучшая модель (из обученных с разным числом итераций) по значению ROUGE на валидационной выборке."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TStUf1SLmh4l"
      },
      "source": [
        "!python eval.py --task=validate --start_from=0005000.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTq3MdGtGE01"
      },
      "source": [
        "Лучшая модель дообучается с использованием RL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu-XLAm8Fe63"
      },
      "source": [
        "# MLE + RL training\n",
        "!python train.py --train_mle=yes --train_rl=yes --mle_weight=0.25 --load_model=0100000.tar --new_lr=0.0001 \n",
        "\n",
        "# RL training\n",
        "!python train.py --train_mle=no --train_rl=yes --mle_weight=0.0 --load_model=0100000.tar --new_lr=0.0001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB-YSeGhGnrK"
      },
      "source": [
        "Модель, обученная только на RL, достигает более высоких показателей ROUGE, но генерирует менее хорошие тексты с точки зрения связности и естественности, поэтому авторы статьи рекомендуют комбинированную стратегию обучения. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVIZJv5uH7i_"
      },
      "source": [
        "* Результаты, приведенные авторами репозитория:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO0bhu9wHgGn"
      },
      "source": [
        "Rouge scores obtained by using best MLE trained model on test set:\n",
        "\n",
        "{\n",
        "'rouge-1': {'f': 0.4412018559893622, 'p': 0.4814799494024485, 'r': 0.4232331027817015}, \n",
        "\n",
        "'rouge-2': {'f': 0.23238981595683728, 'p': 0.2531296070596062, 'r': 0.22407861554997008},\n",
        "\n",
        "'rouge-l': {'f': 0.40477682528278364, 'p': 0.4584684491434479, 'r': 0.40351107200202596}\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeUQzzvNHzPc"
      },
      "source": [
        "Rouge scores obtained by using best MLE + RL trained model on test set:\n",
        "\n",
        "{\n",
        "'rouge-1': {'f': 0.4499047033247696, 'p': 0.4853756369556345, 'r': 0.43544461386607497},\n",
        "\n",
        "'rouge-2': {'f': 0.24037014314625643, 'p': 0.25903387205387235, 'r': 0.23362662645146298},\n",
        "\n",
        "'rouge-l': {'f': 0.41320241732946406, 'p': 0.4616655167980162, 'r': 0.4144419466382236}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiFAGr6aIbIN"
      },
      "source": [
        "* Примеры (article - исходный текст, ref - target summary, dec - сгенерированный моделью текст):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsCPWCkTIa8M"
      },
      "source": [
        "article: russia 's lower house of parliament was scheduled friday to debate an appeal to the prime minister that challenged the right of u.s.-funded radio liberty to operate in russia following its introduction of broadcasts targeting chechnya .\n",
        "\n",
        "ref: russia 's lower house of parliament mulls challenge to radio liberty\n",
        "\n",
        "dec: russian parliament to debate on banning radio liberty\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaW__-pIJBYT"
      },
      "source": [
        "article: continued dialogue with the democratic people 's republic of korea is important although australia 's plan to open its embassy in pyongyang has been shelved because of the crisis over the dprk 's nuclear weapons program , australian foreign minister alexander downer said on friday .\n",
        "\n",
        "ref: dialogue with dprk important says australian foreign minister\n",
        "\n",
        "dec: australian fm says dialogue with dprk important"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkMDr8zAJW3o"
      },
      "source": [
        "article: water levels in the zambezi river are rising due to heavy rains in its catchment area , prompting zimbabwe 's civil protection unit -lrb- cpu -rrb- to issue a flood alert for people living in the zambezi valley , the herald reported on friday .\n",
        "\n",
        "ref: floods loom in zambezi valley\n",
        "\n",
        "dec: water levels rising in zambezi river"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2KBeWCXJvT0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4W38g90JvAS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VcvVX9YJon0"
      },
      "source": [
        "## BERT Extractive Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCOikZ1yJyDo"
      },
      "source": [
        "### Источник:\n",
        "https://deeplearninganalytics.org/text-summarization/\n",
        "\n",
        "https://github.com/nlpyang/BertSum\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPWErqGiSpGZ"
      },
      "source": [
        "Идея: использовать BERT эмбеддинги предложений исходного текста в задаче бинарной классификации для отбора самых значимых предложений, которые войдут в summary.\n",
        "\n",
        "Для получения эмбеддингов нескольких предложений текста перед каждым предложением текста вставляется свой токен начала предложения **[CLS]**, после каждого предложения - символ **[SEP]**. В качестве эмбеддингов сегмента предложения (которые используются для того, чтобы различать первое и второе предложения в парах предложений при обучении  BERT) для последовательности предложений чередуются единичные и нулевые вектора.\n",
        "\n",
        "_[sent1, sent2, sent3, sent4, sent5] -> [EA, EB, EA, EB, EA]._\n",
        "\n",
        "Вектора токенов [CLS] на последнем слое BERT используются в качестве векторов предложений текста. Вектора предложений подаются на вход классификатору (в статье 3 варианта классификации): \n",
        "1. linear layer + sigmoid\n",
        "2. Transformer + sigmoid\n",
        "3. LSTM + sigmoid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_z1kBmMfI0e"
      },
      "source": [
        "![bertsum](bertsum.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgVjUukfgYAr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "1441f504-08a0-4e16-922b-7ab10e1f0291"
      },
      "source": [
        "!pip install --force-reinstall torch==1.1.0"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.1.0\n",
            "  Downloading torch-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (676.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 676.9 MB 3.9 kB/s \n",
            "\u001b[?25hCollecting numpy\n",
            "  Downloading numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 1.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: numpy, torch\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.4 which is incompatible.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.1.0 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.1.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.1.0 which is incompatible.\n",
            "tensorflow 2.3.0 requires numpy<1.19.0,>=1.16.0, but you have numpy 1.21.4 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.21.4 torch-1.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "torch"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3rNua2xJsxN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 813
        },
        "outputId": "1d1a9f9b-bbc8-4921-8d19-a23d79650618"
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 24.1 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 27.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 30 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40 kB 17.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61 kB 12.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 123 kB 13.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.62.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.21.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.20.10-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 46.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.6 MB/s \n",
            "\u001b[?25hCollecting botocore<1.24.0,>=1.23.10\n",
            "  Downloading botocore-1.23.10-py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 27.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.10->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 49.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.10->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2021.10.8)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 48.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.20.10 botocore-1.23.10 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.5.0 urllib3-1.25.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px7VPlV8YOxq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d672d95-dd30-448f-a1c2-b57873e4443d"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 17.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 51 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 71 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 12.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 92 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 102 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 112 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 122 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 124 kB 12.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.4)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfCY-rAGYy5E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "841a3498-9a07-4f6c-c2e0-f63013d1af61"
      },
      "source": [
        "!pip install pyrouge\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyrouge\n",
            "  Downloading pyrouge-0.1.3.tar.gz (60 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▍                          | 10 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 20 kB 28.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 30 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 40 kB 19.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 51 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 60 kB 4.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyrouge\n",
            "  Building wheel for pyrouge (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyrouge: filename=pyrouge-0.1.3-py3-none-any.whl size=191620 sha256=f6fe7e403b919f9e6f4ef49829728e2ee22b65689c565c66fe6116d92149f8bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/35/6a/ffb9a1f51b2b00fee42e7f67f5a5d8e10c67d048cda09ccd57\n",
            "Successfully built pyrouge\n",
            "Installing collected packages: pyrouge\n",
            "Successfully installed pyrouge-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX0FyN5nY9le",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d0abfd4-a938-410e-8c64-a4382efcf808"
      },
      "source": [
        "!pip install multiprocess"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (0.70.12.2)\n",
            "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.7/dist-packages (from multiprocess) (0.3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5M17ucVaPkf"
      },
      "source": [
        "### Данные"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpH5yBI4ckF_"
      },
      "source": [
        "Датасет CNN and Daily Mail \n",
        "\n",
        "Загрузим предобработанные данные."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77qiS30hafOT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10c7b21c-574f-4638-d784-a1e3c8b4e6d7"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uXL_7cVkV-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab178749-0529-4e4f-c44f-bcea7507f500"
      },
      "source": [
        "!wget --no-check-certificate --load-cookies /tmp/cookies.txt \"http://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'http://docs.google.com/uc?export=download&id=1x0d61LP9UAN389YN00z0Pv-7jQgirVg6' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1x0d61LP9UAN389YN00z0Pv-7jQgirVg6\" -O bertsum_data.zip && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-22 08:23:55--  http://docs.google.com/uc?export=download&confirm=auXP&id=1x0d61LP9UAN389YN00z0Pv-7jQgirVg6\n",
            "Resolving docs.google.com (docs.google.com)... 173.194.76.139, 173.194.76.101, 173.194.76.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.76.139|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://docs.google.com/uc?export=download&confirm=auXP&id=1x0d61LP9UAN389YN00z0Pv-7jQgirVg6 [following]\n",
            "--2021-11-22 08:23:55--  https://docs.google.com/uc?export=download&confirm=auXP&id=1x0d61LP9UAN389YN00z0Pv-7jQgirVg6\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.76.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-a0-docs.googleusercontent.com/docs/securesc/2ufjphberrs5da9oeffnr9fn8t50he1s/k37aachatfj2437tgmeiqc8nuhg77vbu/1637569425000/02403291851892694101/01400197822403106198Z/1x0d61LP9UAN389YN00z0Pv-7jQgirVg6?e=download [following]\n",
            "--2021-11-22 08:23:55--  https://doc-04-a0-docs.googleusercontent.com/docs/securesc/2ufjphberrs5da9oeffnr9fn8t50he1s/k37aachatfj2437tgmeiqc8nuhg77vbu/1637569425000/02403291851892694101/01400197822403106198Z/1x0d61LP9UAN389YN00z0Pv-7jQgirVg6?e=download\n",
            "Resolving doc-04-a0-docs.googleusercontent.com (doc-04-a0-docs.googleusercontent.com)... 74.125.133.132, 2a00:1450:400c:c07::84\n",
            "Connecting to doc-04-a0-docs.googleusercontent.com (doc-04-a0-docs.googleusercontent.com)|74.125.133.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=9b5co2v8np6i4&continue=https://doc-04-a0-docs.googleusercontent.com/docs/securesc/2ufjphberrs5da9oeffnr9fn8t50he1s/k37aachatfj2437tgmeiqc8nuhg77vbu/1637569425000/02403291851892694101/01400197822403106198Z/1x0d61LP9UAN389YN00z0Pv-7jQgirVg6?e%3Ddownload&hash=hh4pk392fjgmhm99hs5etrt0d8lbebur [following]\n",
            "--2021-11-22 08:23:55--  https://docs.google.com/nonceSigner?nonce=9b5co2v8np6i4&continue=https://doc-04-a0-docs.googleusercontent.com/docs/securesc/2ufjphberrs5da9oeffnr9fn8t50he1s/k37aachatfj2437tgmeiqc8nuhg77vbu/1637569425000/02403291851892694101/01400197822403106198Z/1x0d61LP9UAN389YN00z0Pv-7jQgirVg6?e%3Ddownload&hash=hh4pk392fjgmhm99hs5etrt0d8lbebur\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.76.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-04-a0-docs.googleusercontent.com/docs/securesc/2ufjphberrs5da9oeffnr9fn8t50he1s/k37aachatfj2437tgmeiqc8nuhg77vbu/1637569425000/02403291851892694101/01400197822403106198Z/1x0d61LP9UAN389YN00z0Pv-7jQgirVg6?e=download&nonce=9b5co2v8np6i4&user=01400197822403106198Z&hash=mov1ciabt5ffvdotvigq3nrdu6snikqa [following]\n",
            "--2021-11-22 08:23:56--  https://doc-04-a0-docs.googleusercontent.com/docs/securesc/2ufjphberrs5da9oeffnr9fn8t50he1s/k37aachatfj2437tgmeiqc8nuhg77vbu/1637569425000/02403291851892694101/01400197822403106198Z/1x0d61LP9UAN389YN00z0Pv-7jQgirVg6?e=download&nonce=9b5co2v8np6i4&user=01400197822403106198Z&hash=mov1ciabt5ffvdotvigq3nrdu6snikqa\n",
            "Connecting to doc-04-a0-docs.googleusercontent.com (doc-04-a0-docs.googleusercontent.com)|74.125.133.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 869392410 (829M) [application/zip]\n",
            "Saving to: ‘bertsum_data.zip’\n",
            "\n",
            "bertsum_data.zip    100%[===================>] 829.12M   135MB/s    in 6.3s    \n",
            "\n",
            "2021-11-22 08:24:03 (131 MB/s) - ‘bertsum_data.zip’ saved [869392410/869392410]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK31E4MAdCEH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d842b9c-160b-470c-afaf-ef2a51548b4d"
      },
      "source": [
        "!git clone https://github.com/nlpyang/BertSum"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BertSum'...\n",
            "remote: Enumerating objects: 301, done.\u001b[K\n",
            "remote: Total 301 (delta 0), reused 0 (delta 0), pack-reused 301\u001b[K\n",
            "Receiving objects: 100% (301/301), 15.03 MiB | 12.42 MiB/s, done.\n",
            "Resolving deltas: 100% (174/174), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ERlE4EYdN9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d24db08-cf7c-4724-db30-2233d30f8d1d"
      },
      "source": [
        "%cd BertSum"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/BertSum\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo0WKWbWiPJ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ace24be-41c0-4dbb-ad97-8141334c3320"
      },
      "source": [
        "!unzip ../bertsum_data.zip -d ./bert_data"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ../bertsum_data.zip\n",
            "  inflating: ./bert_data/cnndm.test.0.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.test.1.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.test.2.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.test.3.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.test.4.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.test.5.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.0.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.100.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.101.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.102.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.103.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.104.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.105.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.106.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.107.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.108.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.109.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.10.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.110.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.111.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.112.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.113.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.114.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.115.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.116.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.117.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.118.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.119.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.11.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.120.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.121.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.122.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.123.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.124.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.125.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.126.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.127.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.128.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.129.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.12.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.130.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.131.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.132.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.133.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.134.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.135.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.136.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.137.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.138.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.139.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.13.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.140.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.141.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.142.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.143.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.14.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.15.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.16.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.17.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.18.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.19.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.1.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.20.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.21.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.22.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.23.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.24.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.25.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.26.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.27.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.28.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.29.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.2.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.30.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.31.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.32.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.33.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.34.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.35.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.36.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.37.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.38.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.39.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.3.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.40.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.41.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.42.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.43.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.44.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.45.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.46.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.47.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.48.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.49.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.4.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.50.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.51.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.52.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.53.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.54.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.55.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.56.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.57.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.58.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.59.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.5.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.60.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.61.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.62.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.63.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.64.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.65.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.66.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.67.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.68.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.69.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.6.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.70.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.71.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.72.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.73.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.74.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.75.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.76.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.77.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.78.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.79.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.7.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.80.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.81.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.82.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.83.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.84.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.85.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.86.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.87.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.88.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.89.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.8.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.90.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.91.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.92.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.93.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.94.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.95.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.96.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.97.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.98.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.99.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.train.9.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.valid.0.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.valid.1.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.valid.2.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.valid.3.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.valid.4.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.valid.5.bert.pt  \n",
            "  inflating: ./bert_data/cnndm.valid.6.bert.pt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8rHG49yedkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09c4f226-cafe-4c96-c3be-057a96e019a2"
      },
      "source": [
        "%cd src"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/BertSum/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHqhIu_LsNeG"
      },
      "source": [
        "Пример входных данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLDitZ3mnv_J"
      },
      "source": [
        "import torch\n",
        "cnn_test_samp = torch.load(\"/content/BertSum/bert_data/cnndm.test.0.bert.pt\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR9kyPftqoWv"
      },
      "source": [
        "cnn_test_samp0 = cnn_test_samp[0]"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx4X9QDEq_uX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "838ca08d-d585-415a-bc51-c34c197dfb1b"
      },
      "source": [
        "cnn_test_samp0.keys()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['src', 'labels', 'segs', 'clss', 'src_txt', 'tgt_txt'])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RydoDsS6oQ6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b957a2aa-bc17-476c-e496-a3a76a55cef6"
      },
      "source": [
        "print(cnn_test_samp0['clss']) # индексы CLS токенов для предложений входного текста \n",
        "print(cnn_test_samp0['labels']) # таргет метки для предложений (1 - входит в summary, 0 - не входит)\n",
        "print(cnn_test_samp0['segs']) # id сегментов предложений \n",
        "print(cnn_test_samp0['src']) # id слов"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 25, 57, 78, 112, 136, 174, 197, 223, 245, 285, 301, 337, 358, 382, 416, 452]\n",
            "[0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[101, 1037, 2118, 1997, 5947, 3076, 2038, 2351, 3053, 2093, 2706, 2044, 1037, 2991, 1999, 4199, 1999, 1037, 6878, 13742, 2886, 1999, 4199, 1012, 102, 101, 4080, 9587, 29076, 1010, 2322, 1010, 2013, 8904, 3449, 9644, 1010, 4307, 1010, 2018, 2069, 2074, 3369, 2005, 1037, 13609, 2565, 1999, 3304, 2043, 1996, 5043, 3047, 1999, 2254, 1012, 102, 101, 2002, 2001, 10583, 2067, 2000, 3190, 3081, 2250, 10771, 2006, 2233, 2322, 1010, 2021, 2002, 2351, 2006, 4465, 1012, 102, 101, 4080, 9587, 29076, 1010, 2322, 1010, 2013, 8904, 3449, 9644, 1010, 4307, 1010, 1037, 2118, 1997, 5947, 3076, 2038, 2351, 3053, 2093, 2706, 2044, 1037, 2991, 1999, 4199, 1999, 1037, 6878, 13742, 102, 101, 2002, 2001, 2579, 2000, 1037, 2966, 4322, 1999, 1996, 3190, 2181, 1010, 2485, 2000, 2010, 2155, 2188, 1999, 8904, 3449, 9644, 1012, 102, 101, 2002, 2351, 2006, 4465, 2012, 7855, 3986, 2902, 1011, 2966, 19684, 1005, 1055, 2436, 14056, 3581, 18454, 6199, 2319, 2758, 1037, 3426, 1997, 2331, 24185, 1050, 1005, 1056, 2022, 2207, 2127, 6928, 2012, 1996, 5700, 1012, 102, 101, 3988, 2610, 4311, 5393, 1996, 2991, 2001, 2019, 4926, 2021, 4614, 2024, 11538, 1996, 6061, 2008, 9587, 29076, 2001, 20114, 1012, 102, 101, 2006, 4465, 1010, 2010, 5542, 9460, 2626, 3784, 1024, 1036, 2023, 2851, 2026, 5542, 4080, 1005, 1055, 3969, 2001, 4196, 2039, 2000, 6014, 1012, 102, 101, 3988, 2610, 4311, 5393, 1996, 2991, 2001, 2019, 4926, 2021, 4614, 2024, 11538, 1996, 6061, 2008, 9587, 29076, 2001, 20114, 102, 101, 1036, 2012, 1996, 2927, 1997, 2254, 2002, 2253, 2000, 4199, 2000, 2817, 7548, 1998, 2006, 1996, 2126, 2188, 2013, 1037, 2283, 2002, 2001, 23197, 4457, 1998, 6908, 2125, 1037, 2871, 6199, 2958, 1998, 2718, 1996, 5509, 2917, 1012, 102, 101, 1036, 2002, 2001, 1999, 1037, 16571, 1998, 1999, 4187, 4650, 2005, 2706, 1012, 1005, 102, 101, 13723, 20073, 1010, 2040, 2056, 2016, 2003, 1037, 2485, 2155, 2767, 1010, 2409, 2026, 9282, 2166, 1010, 2008, 9587, 29076, 2018, 2069, 2042, 1999, 1996, 2406, 2005, 2416, 2847, 2043, 1996, 5043, 3047, 1012, 102, 101, 2016, 2056, 2002, 2001, 2001, 2894, 2012, 1996, 2051, 1997, 1996, 6884, 6101, 1998, 3167, 5167, 2020, 7376, 1012, 102, 101, 2016, 2794, 2008, 2002, 2001, 1999, 1037, 2512, 1011, 2966, 2135, 10572, 16571, 1010, 2383, 4265, 3809, 8985, 1998, 4722, 9524, 1012, 102, 101, 9587, 29076, 2001, 1037, 2353, 1011, 2095, 5446, 2350, 2013, 8904, 3449, 9644, 1010, 5665, 1012, 1010, 2040, 2001, 8019, 1999, 1037, 13609, 1011, 2146, 2565, 2012, 2198, 9298, 4140, 2118, 1012, 102, 101, 9587, 29076, 6272, 2000, 1996, 2082, 1005, 1055, 3127, 1997, 1996, 13201, 16371, 13577, 1010, 4311, 1996, 3190, 10969, 2040, 6866, 1037, 3696, 2648, 1037, 2311, 3752, 1036, 11839, 2005, 9587, 29076, 1012, 1005, 102, 101, 1996, 13577, 1005, 1055, 5947, 3127, 2623, 4465, 5027, 3081, 10474, 2008, 1037, 3986, 2326, 2097, 2022, 2218, 2006, 3721, 2000, 3342, 9587, 29076, 1012, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xs7zKGfYrM0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f134a39-bc78-4912-b897-4b5c483faa69"
      },
      "source": [
        "cnn_test_samp0['src_txt'] # входной текст\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a university of iowa student has died nearly three months after a fall in rome in a suspected robbery attack in rome .',\n",
              " 'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program in italy when the incident happened in january .',\n",
              " 'he was flown back to chicago via air ambulance on march 20 , but he died on sunday .',\n",
              " 'andrew mogni , 20 , from glen ellyn , illinois , a university of iowa student has died nearly three months after a fall in rome in a suspected robbery',\n",
              " 'he was taken to a medical facility in the chicago area , close to his family home in glen ellyn .',\n",
              " \"he died on sunday at northwestern memorial hospital - medical examiner 's office spokesman frank shuftan says a cause of death wo n't be released until monday at the earliest .\",\n",
              " 'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed .',\n",
              " \"on sunday , his cousin abby wrote online : ` this morning my cousin andrew 's soul was lifted up to heaven .\",\n",
              " 'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed',\n",
              " '` at the beginning of january he went to rome to study aboard and on the way home from a party he was brutally attacked and thrown off a 40ft bridge and hit the concrete below .',\n",
              " \"` he was in a coma and in critical condition for months . '\",\n",
              " 'paula barnett , who said she is a close family friend , told my suburban life , that mogni had only been in the country for six hours when the incident happened .',\n",
              " 'she said he was was alone at the time of the alleged assault and personal items were stolen .',\n",
              " 'she added that he was in a non-medically induced coma , having suffered serious infection and internal bleeding .',\n",
              " 'mogni was a third-year finance major from glen ellyn , ill. , who was participating in a semester-long program at john cabot university .',\n",
              " \"mogni belonged to the school 's chapter of the sigma nu fraternity , reports the chicago tribune who posted a sign outside a building reading ` pray for mogni . '\",\n",
              " \"the fraternity 's iowa chapter announced sunday afternoon via twitter that a memorial service will be held on campus to remember mogni .\"]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DSbhBlXrRWH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "0dab8278-ed9a-43e1-f18b-6ec20cb9c4e8"
      },
      "source": [
        "cnn_test_samp0['tgt_txt'] # target summary"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program when the incident happened in january<q>he was flown back to chicago via air on march 20 but he died on sunday<q>initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed<q>his cousin claims he was attacked and thrown 40ft from a bridge'"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "othq3BNHfI33"
      },
      "source": [
        "Обучение модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW5EKEL8d9GG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ebf9785-7269-4c82-f83d-33127ef08f96"
      },
      "source": [
        "!python train.py -mode train -encoder classifier -dropout 0.1 -bert_data_path ../bert_data/cnndm -model_path ../models/bert_classifier -lr 2e-3 -visible_gpus 0  -gpu_ranks 0 -world_size 1 -report_every 50 -save_checkpoint_steps 1000 -batch_size 3000 -decay_method noam -train_steps 2000 -accum_count 2 -log_file ../logs/bert_classifier -use_interval true -warmup_steps 10000"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2021-11-22 08:27:18,635 INFO] Device ID 0\n",
            "[2021-11-22 08:27:18,635 INFO] Device cuda\n",
            "[2021-11-22 08:27:19,044 INFO] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz not found in cache, downloading to /tmp/tmp50le6k85\n",
            "100% 407873900/407873900 [00:14<00:00, 29039317.84B/s]\n",
            "[2021-11-22 08:27:33,544 INFO] copying /tmp/tmp50le6k85 to cache at ../temp/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "[2021-11-22 08:27:34,495 INFO] creating metadata file for ../temp/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "[2021-11-22 08:27:34,496 INFO] removing temp file /tmp/tmp50le6k85\n",
            "[2021-11-22 08:27:34,557 INFO] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at ../temp/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "[2021-11-22 08:27:34,558 INFO] extracting archive file ../temp/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpgut4p8cz\n",
            "[2021-11-22 08:27:39,350 INFO] Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[2021-11-22 08:27:48,849 INFO] Summarizer(\n",
            "  (bert): Bert(\n",
            "    (model): BertModel(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): BertLayerNorm()\n",
            "        (dropout): Dropout(p=0.1)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (encoder): Classifier(\n",
            "    (linear1): Linear(in_features=768, out_features=1, bias=True)\n",
            "    (sigmoid): Sigmoid()\n",
            "  )\n",
            ")\n",
            "gpu_rank 0\n",
            "[2021-11-22 08:27:48,860 INFO] * number of parameters: 109483009\n",
            "[2021-11-22 08:27:48,861 INFO] Start training...\n",
            "[2021-11-22 08:27:48,974 INFO] Loading train dataset from ../bert_data/cnndm.train.123.bert.pt, number of examples: 2001\n",
            "[2021-11-22 08:29:35,138 INFO] Step 50/ 2000; xent: 7.33; lr: 0.0000001;   9 docs/s;    106 sec\n",
            "[2021-11-22 08:31:23,536 INFO] Step 100/ 2000; xent: 6.35; lr: 0.0000002;   9 docs/s;    215 sec\n",
            "[2021-11-22 08:33:12,635 INFO] Step 150/ 2000; xent: 4.88; lr: 0.0000003;   9 docs/s;    324 sec\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 340, in <module>\n",
            "    train(args, device_id)\n",
            "  File \"train.py\", line 272, in train\n",
            "    trainer.train(train_iter_fct, args.train_steps)\n",
            "  File \"/content/BertSum/src/models/trainer.py\", line 157, in train\n",
            "    report_stats)\n",
            "  File \"/content/BertSum/src/models/trainer.py\", line 325, in _gradient_accumulation\n",
            "    (loss/loss.numel()).backward()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/tensor.py\", line 107, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 93, in backward\n",
            "    allow_unreachable=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DqPAkaWfI4B"
      },
      "source": [
        "Тестирование на валидационных и тестовых данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRHkfCKHe0d7"
      },
      "source": [
        "!python train.py -mode validate -bert_data_path ../bert_data/cnndm -model_path ../models/bert_classifier  -visible_gpus 0  -gpu_ranks 0 -batch_size 30000  -log_file ../logs/bert_classifier_valid  -result_path ../results/cnndm -test_all -block_trigram true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD3hXFOMfI4M"
      },
      "source": [
        "Примеры summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mVQNPzaHBDZ"
      },
      "source": [
        "# extracted summary\n",
        "N = 20\n",
        "with open(\"/BertSum/results/cnndm_step2000.candidate\") as f:\n",
        "    dec = [next(f) for x in range(N)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONnGsk_6YC8T"
      },
      "source": [
        "# target summary\n",
        "N = 20\n",
        "with open(\"/BertSum/results/cnndm_step2000.gold\") as f:\n",
        "    ref = [next(f) for x in range(N)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTUMy-7lTROZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f124ac1-1118-4c97-fa4a-ce1f6ae81ce0"
      },
      "source": [
        "ref[0].split('<q>')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the 79th masters tournament gets underway at augusta national on thursday',\n",
              " 'rory mcilroy and tiger woods will be the star attractions in the field bidding for the green jacket at 2015 masters',\n",
              " 'mcilroy , justin rose , ian poulter , graeme mcdowell and more gave sportsmail the verdict on each hole at augusta',\n",
              " 'click on the brilliant interactive graphic below for details on each hole of the masters 2015 course',\n",
              " 'click here for all the latest news from the masters 2015\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vs-rkv00TNma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2339272-b44c-474c-8915-2c5d21490053"
      },
      "source": [
        "dec[0].split('<q>')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['to help get you in the mood for the first major of the year , rory mcilroy , ian poulter , graeme mcdowell and justin rose , plus past masters champions nick faldo and charl schwartzel , give the lowdown on every hole at the world-famous augusta national golf club .',\n",
              " 'the masters 2015 is almost here .',\n",
              " 'click on the graphic below to get a closer look at what the biggest names in the game will face when they tee off on thursday .\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QblpOaQIYdII",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc002b3a-e4ce-43ce-e1a0-d6d7da99a172"
      },
      "source": [
        "ref[1].split('<q>')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"jeff powell looks ahead to saturday 's fight at the mgm grand\",\n",
              " 'floyd mayweather takes on manny pacquiao in $ 300m showdown',\n",
              " 'both fighters arrived in las vegas on tuesday with public appearances',\n",
              " 'read : mayweather makes official arrival ahead of manny pacquiao fight',\n",
              " 'al haymon : the man behind mayweather who is revolutionising boxing',\n",
              " \"mayweather vs pacquiao takes centre stage ... but who 's on the undercard ?\\n\"]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FpWNGk_U4E8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7b280a5-df81-43da-baca-2a098137e037"
      },
      "source": [
        "dec[1].split('<q>')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"powell reflects on the pair 's arrivals on the las vegas strip and looks forward to the rest of the week .\",\n",
              " 'both boxers made public appearances on tuesday as their $ 300million showdown draws ever closer , and our man powell was there .',\n",
              " \"sportsmail 's boxing correspondent jeff powell looks ahead to saturday 's mega-fight at the mgm grand after witnessing floyd mayweather and manny pacquiao 's grand arrivals in las vegas .\\n\"]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZCaWmqEYgLN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0446d2b-1835-45e2-e0c8-1ef292ef3389"
      },
      "source": [
        "ref[2].split('<q>')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gary locke has been interim manager since start of february',\n",
              " 'locke has won two and drawn four of his seven games in charge',\n",
              " 'the 37-year-old took over when allan johnston quit\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMiC_NqjYpQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5cdcfa5-e110-461a-b1df-8e1921f434e4"
      },
      "source": [
        "dec[2].split('<q>')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the former hearts boss joined the club as assistant boss to allan johnston last summer but took control of the team when his ex-tynecastle team-mate quit at the start of february .',\n",
              " 'gary locke has been given the job at kilmarnock on a permanent basis after a successful interim spell',\n",
              " 'the 39-year-old - who will speak at a press conference on friday morning - has lost just once in seven games since taking over at rugby park .\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSkCIBD_YsaR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}